{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - torchvision\n",
      "  - torchaudio\n",
      "  - pytorch\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/pytorch/win-32\n",
      "  - https://conda.anaconda.org/pytorch/noarch\n",
      "  - https://repo.anaconda.com/pkgs/main/win-32\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/win-32\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "  - https://repo.anaconda.com/pkgs/msys2/win-32\n",
      "  - https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.8.3\n",
      "System: Windows\n",
      "System Version: 10.0.22621\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "# Get Python version\n",
    "python_version = platform.python_version()\n",
    "\n",
    "# Get system information\n",
    "system_info = platform.system()\n",
    "system_version = platform.version()\n",
    "\n",
    "print(f\"Python Version: {python_version}\")\n",
    "print(f\"System: {system_info}\")\n",
    "print(f\"System Version: {system_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\basil\\anaconda3\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\basil\\anaconda3\\lib\\site-packages (from gymnasium) (4.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0; python_version < \"3.10\" in c:\\users\\basil\\anaconda3\\lib\\site-packages (from gymnasium) (7.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\basil\\anaconda3\\lib\\site-packages (from gymnasium) (1.24.4)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\basil\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\basil\\anaconda3\\lib\\site-packages (from gymnasium) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\basil\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0; python_version < \"3.10\"->gymnasium) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-731b441832b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import necessary libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Define the Q-network class using PyTorch\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Define the Deep Q-Network (DQN) agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size=258, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "        # Initialize agent parameters\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # Initialize Q-network model and target model using the QNetwork class\n",
    "        self.model = QNetwork(state_size, hidden_size, action_size)\n",
    "        self.target_model = QNetwork(state_size, hidden_size, action_size)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        # Initialize Adam optimizer and mean squared error loss function\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Store experience tuple (state, action, reward, next_state, done) in replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # Train on the whole memory ? Can cause overfitting ? \n",
    "        #minibatch = np.array(self.memory)\n",
    "        \n",
    "        # If not enough memory, do nothing\n",
    "        if len(self.memory)<batch_size:\n",
    "          return\n",
    "        minibatch = np.array(random.sample(self.memory, batch_size))\n",
    "\n",
    "        states = np.vstack(minibatch[:, 0])\n",
    "        actions = np.array(minibatch[:, 1], dtype=np.int64)\n",
    "        rewards = np.array(minibatch[:, 2], dtype=np.int64)\n",
    "        next_states = np.vstack(minibatch[:, 3])\n",
    "        dones = np.array(minibatch[:, 4], dtype=np.int64)\n",
    "\n",
    "        # Convert NumPy arrays to PyTorch tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q-values for the current state and selected actions\n",
    "        Q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        # Compute target Q-values for the next state using the target network\n",
    "        next_Q_values = self.target_model(next_states).max(dim=1).values.detach()\n",
    "        target_Q_values = rewards + (1 - dones) * self.gamma * next_Q_values\n",
    "\n",
    "        # Update the Q-network using MSE\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_function(Q_values, target_Q_values.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update epsilon (the more the agent plays, the less we want him to take a random action)\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # Update the target model by copying the parameters from the current model\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "# Define the training function for the DQN agent\n",
    "def train_dqn(agent, env, episodes=1000, batch_size=64):\n",
    "    for episode in range(episodes):\n",
    "        # Reset the environment and initialize variables for the current episode\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Run the episode until termination\n",
    "        while not done:\n",
    "            # Select an action, take a step, and store the experience in replay memory\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        # Perform a Q-learning update using replay memory\n",
    "        agent.replay(batch_size)\n",
    "        # Update the target model to track the changes in the Q-network\n",
    "        agent.update_target_model()\n",
    "\n",
    "        # Print episode information every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the CartPole environment\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # Initialize the DQN agent\n",
    "    dqn_agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    # Train the DQN agent on the CartPole environment\n",
    "    train_dqn(dqn_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
